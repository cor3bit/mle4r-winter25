{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer and Loss"
      ],
      "metadata": {
        "id": "7_PA03GbeyIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Metrics"
      ],
      "metadata": {
        "id": "ZNP3CvLjeyS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the required packages"
      ],
      "metadata": {
        "id": "SHFmRS8zeyVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install seaborn"
      ],
      "metadata": {
        "id": "Q30A61o_e4Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the imports"
      ],
      "metadata": {
        "id": "ji1Y9b6efPGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "EtWrCIJde4W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Picking a Loss Function"
      ],
      "metadata": {
        "id": "PuFX0VcgfZyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with **regression**. Key thing here is how much we want to penalize the outliers."
      ],
      "metadata": {
        "id": "fbo23BjfaUuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given residulas $r = \\hat{y} - y$, let's try several options"
      ],
      "metadata": {
        "id": "QZll_bd2covt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define error range\n",
        "residuals = np.linspace(-5, 5, 100)\n",
        "\n",
        "# Define different loss functions\n",
        "mae = np.abs(residuals)  # Mean Absolute Error\n",
        "mse = residuals**2  # Mean Squared Error\n",
        "rmse = np.sqrt(residuals**2)  # Root Mean Squared Error (same shape as MAE)\n",
        "huber_d1 = np.where(np.abs(residuals) <= 1, 0.5 * residuals**2, np.abs(residuals) - 0.5)  # Huber loss with delta=1\n",
        "huber_d10 = np.where(np.abs(residuals) <= 10, 0.5 * residuals**2, 10 * (np.abs(residuals) - 5))  # Huber loss with delta=10\n",
        "logcosh = np.log(np.cosh(residuals))  # Log-Cosh Loss\n",
        "\n",
        "# Plot\n",
        "# plt.figure(figsize=(8, 6))\n",
        "plt.plot(residuals, mae, label=\"MAE\")\n",
        "plt.plot(residuals, mse, label=\"MSE\")\n",
        "plt.plot(residuals, rmse, label=\"RMSE\")\n",
        "plt.plot(residuals, huber_d1, label=\"Huber (d=1)\")\n",
        "plt.plot(residuals, huber_d10, label=\"Huber (d=10)\")\n",
        "plt.plot(residuals, logcosh, label=\"LogCosh\")\n",
        "\n",
        "plt.ylim(0, 4)  # Set y-limit for better visualization\n",
        "plt.xlabel(\"Residual\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss Functions for Regression\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wJ8CjXvHfevy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For **classification** the choice is how we handle the edge cases: when $-log(\\hat{y})$ (prediction probability for the correct in a binary classification case) is near 0 and 1."
      ],
      "metadata": {
        "id": "Jrzjl8gQfoW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "probs = np.linspace(0.001, 0.999, 100)\n",
        "\n",
        "# Compute different loss functions\n",
        "log_loss = -np.log(probs)  # -log(x), standard log loss for correct class\n",
        "hinge_loss = np.maximum(0, 1 - probs)  # Hinge loss for binary classification\n",
        "squared_hinge_loss = (np.maximum(0, 1 - probs)) ** 2  # Squared Hinge loss\n",
        "logcosh_loss = np.log(np.cosh(1 - probs))  # Log-Cosh Loss\n",
        "\n",
        "# Plot\n",
        "# plt.figure(figsize=(8, 6))\n",
        "plt.plot(probs, log_loss, label=\"-log(x) (Binary Cross-Entropy)\", color=\"blue\")\n",
        "plt.plot(probs, hinge_loss, label=\"Hinge Loss\", color=\"red\")\n",
        "plt.plot(probs, squared_hinge_loss, label=\"Squared Hinge Loss\", color=\"green\")\n",
        "plt.plot(probs, logcosh_loss, label=\"LogCosh Loss\", color=\"purple\")\n",
        "\n",
        "plt.xlabel(\"Probability of Correct Class (x)\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss Functions for Classification\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uMyOC0eDeIu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Functions Matter!"
      ],
      "metadata": {
        "id": "DiQzqsbpkhEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See https://arxiv.org/abs/2204.12511"
      ],
      "metadata": {
        "id": "PbNEB4buklQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JAX: Loss Functions for Regression"
      ],
      "metadata": {
        "id": "QAP7NGhyZ1d0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax"
      ],
      "metadata": {
        "id": "8xnzpf2-aJqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  See https://optax.readthedocs.io/en/latest/api/losses.html\n",
        "   - `optax.squared_error()` → **MSE**\n",
        "   - `jnp.abs(y_pred - y_true).mean()` → **MAE**\n",
        "   - `optax.huber_loss()` → **Huber Loss**"
      ],
      "metadata": {
        "id": "RL-BX9zpmB9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = jnp.array([3.0, -0.5, 2.0, 7.0])\n",
        "y_pred = jnp.array([2.5, 0.0, 2.0, 8.0])\n",
        "\n",
        "mse_loss = optax.squared_error(y_pred, y_true).mean()\n",
        "mae_loss = jnp.abs(y_pred - y_true).mean()\n",
        "huber_loss = optax.huber_loss(y_pred, y_true).mean()\n",
        "\n",
        "print(f\"MSE Loss: {mse_loss:.4f}\")\n",
        "print(f\"MAE Loss: {mae_loss:.4f}\")\n",
        "print(f\"Huber Loss: {huber_loss:.4f}\")"
      ],
      "metadata": {
        "id": "-jBMlkilaJnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JAX: Loss Functions for Classification"
      ],
      "metadata": {
        "id": "v2P9ZboDl4U0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For classification, we distinguish between:  \n",
        "- **Raw logits** (unscaled model outputs)  \n",
        "- **Probabilities** (logits transformed using softmax or sigmoid)  \n",
        "- **Labels** (integer class indices)  \n",
        "- **One-hot encoded labels** (binary vector representation of class labels)\n",
        "\n",
        "Loss function signature expects a particular value."
      ],
      "metadata": {
        "id": "IduSHmFAn0UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = jnp.array([1.0, -1.5, 0.8, -0.3])\n",
        "labels = jnp.array([1, 0, 1, 0])\n",
        "\n",
        "# Logits vs Labels [0,1]\n",
        "bce_loss = optax.sigmoid_binary_cross_entropy(logits, labels).mean()\n",
        "\n",
        "# !!! Logits vs Labels [-1,1]\n",
        "labels_hinge = jnp.array([1, -1, 1, -1])\n",
        "hinge_loss = optax.hinge_loss(logits, labels_hinge).mean()\n",
        "\n",
        "print(f\"Binary Cross-Entropy Loss: {bce_loss:.4f}\")\n",
        "print(f\"Hinge Loss: {hinge_loss:.4f}\")"
      ],
      "metadata": {
        "id": "I9AAwdHQaQRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizers"
      ],
      "metadata": {
        "id": "Ijoze0AwZx7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple SGD"
      ],
      "metadata": {
        "id": "isGvnJA-qaio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1D quadratic function\n",
        "def loss_fn(w):\n",
        "    return (w - 3) ** 2"
      ],
      "metadata": {
        "id": "o3lHg5nVq2Am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad_fn = jax.grad(loss_fn)"
      ],
      "metadata": {
        "id": "0zomUqGBrF8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "\n",
        "w = jnp.array(0.0)\n",
        "print(f\"Start: w = {w:.4f}, Loss = {loss_fn(w):.4f}\")\n",
        "\n",
        "for i in range(20):\n",
        "    grad = grad_fn(w)  # Compute gradient dL/dw\n",
        "    w = w - learning_rate * grad  # SGD update step\n",
        "    print(f\"Iteration {i+1}: w = {w:.4f}, Loss = {loss_fn(w):.4f}\")\n",
        "\n",
        "print(\"\\nFinal optimized weight:\", w)"
      ],
      "metadata": {
        "id": "eHkd54IZqcNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accelaration SGD"
      ],
      "metadata": {
        "id": "RvmZF8x6tJAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Several ways to accelerate:\n",
        "- **Momentum SGD**  \n",
        "  $$\n",
        "  v_{t+1} = \\beta v_t + (1 - \\beta) \\nabla L(w_t)\n",
        "  $$\n",
        "  $$\n",
        "  w_{t+1} = w_t - \\eta v_{t+1}\n",
        "  $$\n",
        "  - Accumulates past gradients to **dampen oscillations**.\n",
        "  - Helps move faster in **low-curvature directions**.\n",
        "  - Example: `optax.sgd(momentum=0.9)`\n",
        "\n",
        "\n",
        "- **Adagrad**  \n",
        "  $$\n",
        "  g_t = \\nabla L(w_t)\n",
        "  $$\n",
        "  $$\n",
        "  G_t = G_{t-1} + g_t^2\n",
        "  $$\n",
        "  $$\n",
        "  w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{G_t} + \\epsilon} g_t\n",
        "  $$\n",
        "  - Adjusts learning rates per-parameter.\n",
        "  - **Good for sparse data** (e.g., NLP, embeddings).\n",
        "  - Downside: **Aggressively decays learning rate** over time."
      ],
      "metadata": {
        "id": "gLh7zui3sFZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "display(Image(url=\"https://www.ruder.io/content/images/2016/09/contours_evaluation_optimizers.gif\"))"
      ],
      "metadata": {
        "id": "DBCrRux1tt48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "display(Image(url=\"https://www.ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif\"))\n"
      ],
      "metadata": {
        "id": "JZnpDDuAslrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://www.ruder.io/optimizing-gradient-descent/"
      ],
      "metadata": {
        "id": "hnHwm5I1tfmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Optax"
      ],
      "metadata": {
        "id": "y95N-bbFqaWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1️⃣ **Define an optimizer**\n",
        "```python\n",
        "opt = optax.adam(learning_rate=0.001)\n",
        "```\n",
        "2️⃣ **Initialize optimizer state**\n",
        "```python\n",
        "params = jnp.array(0.0)\n",
        "opt_state = opt.init(params)\n",
        "```\n",
        "3️⃣ **Compute gradients**\n",
        "```python\n",
        "grad = jax.grad(loss_fn)(params)\n",
        "```\n",
        "4️⃣ **Update parameters**\n",
        "```python\n",
        "updates, opt_state = opt.update(grad, opt_state)\n",
        "params = optax.apply_updates(params, updates)\n",
        "```"
      ],
      "metadata": {
        "id": "0CK4k1TBw6zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizers = {\n",
        "    \"SGD\": optax.sgd(learning_rate=0.1),\n",
        "    \"Momentum\": optax.sgd(learning_rate=0.1, momentum=0.9),\n",
        "    \"Adam\": optax.adam(learning_rate=0.1),\n",
        "    \"RMSprop\": optax.rmsprop(learning_rate=0.1)\n",
        "}\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "for opt_name, optimizer in optimizers.items():\n",
        "    print(f\"\\n🔹 Using {opt_name} Optimizer 🔹\")\n",
        "\n",
        "    # Initialize weight\n",
        "    w = jnp.array(0.0)\n",
        "\n",
        "    # Initialize Optax optimizer state\n",
        "    opt_state = optimizer.init(w)\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(num_epochs):\n",
        "        grad = grad_fn(w)  # Compute gradient\n",
        "\n",
        "        # Compute optimizer update\n",
        "        updates, opt_state = optimizer.update(grad, opt_state, w)\n",
        "\n",
        "        # Apply update to w\n",
        "        w = optax.apply_updates(w, updates)\n",
        "\n",
        "        print(f\"Iteration {i+1}: w = {w:.4f}, Loss = {loss_fn(w):.4f}\")\n",
        "\n",
        "    print(f\"✅ Final optimized weight with {opt_name}: {w:.4f}\")"
      ],
      "metadata": {
        "id": "CK3mt7A5qbkx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}