{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "7_PA03GbeyIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: ensure Runtime GPU is selected!"
      ],
      "metadata": {
        "id": "-6elihvG-kS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anatomy of the Model"
      ],
      "metadata": {
        "id": "ZNP3CvLjeyS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the required packages"
      ],
      "metadata": {
        "id": "SHFmRS8zeyVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install flax"
      ],
      "metadata": {
        "id": "Q30A61o_e4Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the imports"
      ],
      "metadata": {
        "id": "ji1Y9b6efPGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from functools import partial\n",
        "from typing import Any, Callable\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax\n",
        "from flax import linen as nn"
      ],
      "metadata": {
        "id": "EtWrCIJde4W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model signature"
      ],
      "metadata": {
        "id": "PuFX0VcgfZyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ f(w; x) = \\hat{y} $$\n",
        "We place parameters at the first place to match the signature required later by JAX."
      ],
      "metadata": {
        "id": "XxPDzUYrcHGe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3gD_xPgzqDN"
      },
      "outputs": [],
      "source": [
        "# Linear Regression\n",
        "np.random.seed(1337)\n",
        "\n",
        "def predict(w, x):\n",
        "  # y = w.T @ x\n",
        "  y = np.sum(w * x)\n",
        "  return y\n",
        "\n",
        "params = np.ones(5)\n",
        "\n",
        "# features, batch of data\n",
        "x = np.array([1] + [2, 3, 7, 2])\n",
        "\n",
        "# output\n",
        "y = predict(params, x)\n",
        "\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP model signature"
      ],
      "metadata": {
        "id": "5FvWhS_odj-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-layer Dense network"
      ],
      "metadata": {
        "id": "_PZy-DdU3zJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import HTML\n",
        "\n",
        "url = \"https://www.researchgate.net/publication/221079407/figure/fig1/AS:651187686744067@1532266651725/One-layer-neural-network-and-nomenclature-employed.png\"\n",
        "display(HTML(f'<img src=\"{url}\" width=\"500px\">'))\n"
      ],
      "metadata": {
        "id": "9_GTVI6Z29gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(W, b, x):\n",
        "    z = W @ x + b   # Linear transformation\n",
        "    a = np.maximum(0, z)  # ReLU activation\n",
        "    return a\n",
        "\n",
        "input_dim = 4  # Input features\n",
        "output_dim = 1  # Number of output neurons\n",
        "\n",
        "x = np.array([2, 3, 7, 2])\n",
        "\n",
        "W = np.ones((output_dim, input_dim))  # Initialize weights with all 1s\n",
        "b = np.ones(output_dim, )  # Initialize biases with all 1s\n",
        "\n",
        "y = predict(W, b, x)\n",
        "\n",
        "y"
      ],
      "metadata": {
        "id": "v4A9AReadi7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP in JAX and Flax"
      ],
      "metadata": {
        "id": "6j1Moeo0djkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX as NumPy on steroids!\n",
        "But beware: https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html"
      ],
      "metadata": {
        "id": "9hzalg3IeZmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "import jax\n",
        "\n",
        "def predict(W, b, x):\n",
        "    z = W @ x + b  # Linear transformation\n",
        "    a = jnp.maximum(0, z)  # ReLU activation\n",
        "    return a\n",
        "\n",
        "input_dim = 4\n",
        "output_dim = 1\n",
        "\n",
        "W = jnp.ones((output_dim, input_dim))\n",
        "b = jnp.ones((output_dim, ))\n",
        "\n",
        "x = jnp.array([2, 3, 7, 2])\n",
        "\n",
        "y = predict(W, b, x)\n",
        "\n",
        "print(y)"
      ],
      "metadata": {
        "id": "zwTRI_SLeY_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flax - a library for Neural Networks in JAX"
      ],
      "metadata": {
        "id": "wK5Ob7IqeooE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flax Model API:\n",
        "\n",
        "1️⃣ **Define the model** (`nn.Module`, (optionally) with `setup()`)  \n",
        "2️⃣ **Initialize parameters** (`model.init()`)  \n",
        "3️⃣ **Run inference** (`model.apply()`)  \n"
      ],
      "metadata": {
        "id": "m6uWuQ3eeSsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import flax.linen as nn\n",
        "\n",
        "class VanillaMLP(nn.Module):\n",
        "    output_dim: int\n",
        "\n",
        "    def setup(self):\n",
        "        self.dense = nn.Dense(\n",
        "            self.output_dim,\n",
        "            kernel_init=lambda key, shape, dtype: jnp.ones(shape, dtype), # just to match the init of previous models\n",
        "            bias_init=lambda key, shape, dtype: jnp.ones(shape, dtype), # just to match the init of previous models\n",
        "        )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        z = self.dense(x)  # Linear transformation\n",
        "        return nn.relu(z)  # ReLU activation\n",
        "\n",
        "input_dim = 4\n",
        "output_dim = 1\n",
        "\n",
        "x = jnp.array([2, 3, 7, 2])\n",
        "\n",
        "model = VanillaMLP(output_dim=output_dim)\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "params = model.init(key, jnp.ones(input_dim))\n",
        "\n",
        "y = model.apply(params, x)\n",
        "\n",
        "print(y)"
      ],
      "metadata": {
        "id": "HojXQSOg5X24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bookkeeping"
      ],
      "metadata": {
        "id": "_DFwnwGN6NSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **Flax**, model parameters (`params`) are stored as a **frozen dictionary (`FrozenDict`)**, which can be **saved and loaded** using JAX serialization tools like `flax.serialization.to_bytes()` and `flax.serialization.from_bytes()`, or `pickle`/`json` for more flexibility.\n",
        "\n",
        "**1️⃣ Save Model Weights to a File**\n",
        "```python\n",
        "import flax\n",
        "import pickle\n",
        "\n",
        "# Save params to a file (binary format)\n",
        "with open(\"model_params.pkl\", \"wb\") as f:\n",
        "    pickle.dump(flax.serialization.to_bytes(params), f)\n",
        "```\n",
        "\n",
        "**2️⃣ Load Model Weights from a File**\n",
        "```python\n",
        "# Load params from file\n",
        "with open(\"model_params.pkl\", \"rb\") as f:\n",
        "    params_loaded = flax.serialization.from_bytes(params, pickle.load(f))\n",
        "\n",
        "print(\"Loaded Parameters:\", params_loaded)\n",
        "```\n"
      ],
      "metadata": {
        "id": "mho3hfCK6WSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"model_params.pkl\", \"wb\") as f:\n",
        "    pickle.dump(flax.serialization.to_bytes(params), f)"
      ],
      "metadata": {
        "id": "DwjJjWBP6OA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"model_params.pkl\", \"rb\") as f:\n",
        "    params_loaded = flax.serialization.from_bytes(params, pickle.load(f))"
      ],
      "metadata": {
        "id": "Kw2tk4gg66ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run inference again\n",
        "y = model.apply(params_loaded, x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "ZCyYTikz6-bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-trained Models"
      ],
      "metadata": {
        "id": "y6IlJXPFfhjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install transformers"
      ],
      "metadata": {
        "id": "TlezWjuDg8IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Image\n",
        "import requests\n",
        "from PIL import Image as PILImage\n",
        "from io import BytesIO\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "\n",
        "# Fetch and display the image\n",
        "response = requests.get(url)\n",
        "img = PILImage.open(BytesIO(response.content))\n",
        "display(img)"
      ],
      "metadata": {
        "id": "k6hCY3_1hVHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model: ViT https://huggingface.co/docs/transformers/en/model_doc/vit\n",
        "\n",
        "Trained on ImageNet: https://paperswithcode.com/dataset/imagenet"
      ],
      "metadata": {
        "id": "86mm3ZGWh7iH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTImageProcessor, ViTForImageClassification\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "# Get the image from the web\n",
        "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "# Load preprocessor and model\n",
        "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "# Run the inference engine\n",
        "inputs = processor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)"
      ],
      "metadata": {
        "id": "cyrdfTv-fgpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(outputs)"
      ],
      "metadata": {
        "id": "5380FLb0ievU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = outputs.logits\n",
        "logits.shape"
      ],
      "metadata": {
        "id": "ydlrYzCiic5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-1\n",
        "predicted_class_idx = logits.argmax(-1).item()\n",
        "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
      ],
      "metadata": {
        "id": "NPY75QFvf_5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Get the top-10 predictions\n",
        "top_10 = torch.topk(logits, 10)\n",
        "\n",
        "# Extract top indices and their corresponding scores\n",
        "top_10_indices = top_10.indices[0].tolist()\n",
        "top_10_scores = top_10.values[0].tolist()\n",
        "\n",
        "# Display results\n",
        "print(\"Top-10 Predicted Classes:\")\n",
        "for rank, (idx, score) in enumerate(zip(top_10_indices, top_10_scores), start=1):\n",
        "    print(f\"{rank}. {model.config.id2label[idx]} ({score:.4f})\")"
      ],
      "metadata": {
        "id": "S6mB2Koaiwz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention and Transformer"
      ],
      "metadata": {
        "id": "D71_dJG1enrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preliminaries"
      ],
      "metadata": {
        "id": "DLQRaWoNrClD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embeddings**\n",
        "\n",
        "Embeddings are a way to convert discrete tokens (like words or indices) into continuous vectors. Instead of representing a word as a huge, sparse one-hot vector (mostly zeros, one one), an embedding maps it into a smaller, dense vector space where similar tokens can have similar representations.\n",
        "\n",
        "**Why use embeddings?**\n",
        "\n",
        "- **Efficiency:** One-hot vectors are high-dimensional and sparse (e.g., a vocabulary of 100 words gives a 100-dimensional vector, with 99 zeros).  \n",
        "- **Expressiveness:** Dense vectors (say, 256 dimensions) can capture more nuanced relationships between words (similar words can end up with similar vectors).  \n",
        "- **Learning:** Embeddings are learnable parameters. Instead of manually designing features, the model learns the best way to represent tokens for the task at hand.\n",
        "\n",
        "**How does it work?**\n",
        "\n",
        "Imagine you have a vocabulary of 100 tokens. A one-hot vector for any token is a vector of length 100 with a single 1 at the token’s index and 0 everywhere else. An embedding layer is essentially a lookup table (a matrix) of shape `(vocab_size, embedding_dim)`. When you \"look up\" a token, you use its one-hot vector to select the corresponding row from this matrix.\n",
        "\n",
        "$$\n",
        "\\text{embedding} = \\mathbf{onehot} \\times E\n",
        "$$\n",
        "\n",
        "Because $ \\mathbf{onehot} $ has all zeros except a one at the token index, this effectively selects the row of $E $ corresponding to that token."
      ],
      "metadata": {
        "id": "AB-tk3d8xCOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10\n",
        "embed_dim = 3\n",
        "\n",
        "# Create a random embedding matrix of shape (10, 3)\n",
        "# Will be learned during training!\n",
        "embedding_matrix = np.random.randn(vocab_size, embed_dim)\n",
        "print(\"Embedding Matrix:\\n\", embedding_matrix)\n",
        "\n",
        "# Let's say our token index is 2\n",
        "token_index = 2\n",
        "\n",
        "# Create one-hot vector for token_index 2\n",
        "one_hot = np.zeros(vocab_size)\n",
        "one_hot[token_index] = 1\n",
        "print(\"One-hot vector:\\n\", one_hot)\n",
        "\n",
        "# Get embedding by dot product: (1,5) * (5,3) = (1,3)\n",
        "embedding = one_hot.dot(embedding_matrix)\n",
        "print(\"Resulting embedding:\\n\", embedding)\n",
        "\n",
        "# Alternatively, simply index the embedding matrix:\n",
        "embedding_lookup = embedding_matrix[token_index]\n",
        "print(\"Embedding via lookup:\\n\", embedding_lookup)"
      ],
      "metadata": {
        "id": "uNZxKaOcy37o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-Head (Dot-Product) Attention**\n",
        "\n",
        "See the original paper https://arxiv.org/abs/1706.03762\n",
        "\n",
        "Why attention? Intuitively, processing the sequence word for word is not always optimal:\n",
        "\n",
        "<img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png\" width=\"500\">\n",
        "\n",
        "Access all sequence elements at each time step!\n",
        "\n",
        "At its core, attention works by:\n",
        "\n",
        "1. **Creating Queries, Keys, and Values:**  \n",
        "   For each token, we create three vectors:\n",
        "   - **Query (Q):** Represents what information the token is looking for in the rest of the sequence.\n",
        "   - **Key (K):** Represents the token’s characteristics that might be useful for other tokens.\n",
        "   - **Value (V):** Contains the actual information of the token.\n",
        "   \n",
        "   In many cases, these vectors are created by applying different learned transformations to the original embedding.\n",
        "\n",
        "2. **Calculating Attention Scores:**  \n",
        "   For a given token, we want to figure out how much attention it should pay to every other token. We do this by comparing its query with the keys of all tokens:\n",
        "   - **Dot Product:** We calculate a dot product between the query of $i$-th token and the key of each token. This measures their similarity.\n",
        "   - **Scaling:** The dot product is scaled (divided by the square root of the dimension of the vectors) to prevent the numbers from getting too large, which helps stabilize learning.\n",
        "\n",
        "3. **Applying Softmax:**  \n",
        "   The raw attention scores are then passed through a softmax function. This converts the scores into probabilities (or weights) that add up to 1. The softmax emphasizes the tokens with higher similarity scores, so $i$-th token will \"attend\" more to those tokens.\n",
        "\n",
        "4. **Aggregating Values:**  \n",
        "   Finally, we use these attention weights to create a new representation for token $i$:\n",
        "   - Multiply each token's value vector by its corresponding attention weight.\n",
        "   - Sum up these weighted vectors.  \n",
        "     \n",
        "   This weighted sum is the attention output for token $i$—it’s a blend of information from all tokens, focused according to the attention weights.\n",
        "\n",
        "\n",
        "<img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png\" width=\"800\">\n",
        "\n",
        "\n",
        "\n",
        "Images: https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention"
      ],
      "metadata": {
        "id": "HLxpTMHIkhPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# ----- Step 1: Define a simple embedding matrix for 5 tokens -----\n",
        "embedding_matrix = np.array([\n",
        "    [0.1, 0.2, 0.3, 0.4],   # Token 0\n",
        "    [0.5, 0.6, 0.7, 0.8],   # Token 1\n",
        "    [0.9, 1.0, 1.1, 1.2],   # Token 2\n",
        "    [1.3, 1.4, 1.5, 1.6],   # Token 3\n",
        "    [1.7, 1.8, 1.9, 2.0]    # Token 4\n",
        "])\n",
        "\n",
        "# Assume our sequence consists of tokens: [1, 2, 3]\n",
        "sequence_ids = [1, 2, 3]\n",
        "\n",
        "embeddings = embedding_matrix[sequence_ids]  # Shape: (num_tokens, embed_dim)\n",
        "\n",
        "# For simplicity, we use the embeddings directly as Q, K, and V\n",
        "Q = embeddings.copy()  # Queries for all tokens\n",
        "K = embeddings.copy()  # Keys for all tokens\n",
        "V = embeddings.copy()  # Values for all tokens\n",
        "\n",
        "# Let's focus on computing the attention for token 2 (which is at index 1 in our sequence)\n",
        "target_index = 1  # This corresponds to token 2\n",
        "\n",
        "# ----- Step 2: Compute the scaled dot product scores -----\n",
        "embed_dim = embeddings.shape[1]\n",
        "num_tokens = embeddings.shape[0]\n",
        "scores = np.zeros(num_tokens)\n",
        "\n",
        "# Compute dot product between Q[target_index] and every key K[j]\n",
        "for j in range(num_tokens):\n",
        "    dot_product = 0.0\n",
        "    for i in range(embed_dim):\n",
        "        dot_product += Q[target_index, i] * K[j, i]\n",
        "\n",
        "    # Scale the score by the square root of the embedding dimension for stability\n",
        "    scores[j] = dot_product / np.sqrt(embed_dim)\n",
        "\n",
        "print(\"Raw attention scores for token 2:\", scores)\n",
        "\n",
        "# ----- Step 3: Apply Softmax to get attention weights -----\n",
        "def softmax(x):\n",
        "    # Subtracting the max for numerical stability\n",
        "    exp_x = np.exp(x - np.max(x))\n",
        "    return exp_x / np.sum(exp_x)\n",
        "\n",
        "attn_weights = softmax(scores)\n",
        "print(\"Attention weights for token 2:\", attn_weights)\n",
        "\n",
        "# ----- Step 4: Compute the attention output for token 2 -----\n",
        "# This is the weighted sum of the value vectors from all tokens\n",
        "attention_output = np.zeros(embed_dim)\n",
        "for j in range(num_tokens):\n",
        "    for i in range(embed_dim):\n",
        "        attention_output[i] += attn_weights[j] * V[j, i]\n",
        "\n",
        "print(\"Attention output for token 2:\", attention_output)"
      ],
      "metadata": {
        "id": "ZoA5Db7C2_kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini Transformer in Flax"
      ],
      "metadata": {
        "id": "AMd-Guea016M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mask\n",
        "jnp.tril(jnp.ones((10, 10)))"
      ],
      "metadata": {
        "id": "wHBzmIdkqGr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb61939e-e9ae-416f-8b75-9ed808be0782_1456x1392.png\" width=\"600\">\n",
        "\n",
        "Image: https://magazine.sebastianraschka.com/p/building-a-gpt-style-llm-classifier\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cz4HvflL56hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NanoLM(nn.Module):\n",
        "    vocab_size: int\n",
        "    num_layers: int = 6\n",
        "    num_heads: int = 8\n",
        "    head_size: int = 32\n",
        "    dropout_rate: float = 0.2\n",
        "    embed_size: int = 256\n",
        "    block_size: int = 64\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, training: bool = True):\n",
        "        # x: (8, 10) -- batch of 8 sequences, each of length 10 (token indices)\n",
        "\n",
        "        seq_len = x.shape[1] # seq_len = 10\n",
        "\n",
        "        # from index in the vocab to a dense vector\n",
        "        # Output: (8, 10, 256)\n",
        "        x = nn.Embed(self.vocab_size, self.embed_size)(x)\n",
        "\n",
        "        # positional embedding\n",
        "        # Output: (10, 256), then broadcast to (8, 10, 256) for addition\n",
        "        x = x + nn.Embed(self.block_size, self.embed_size)(jnp.arange(seq_len))\n",
        "\n",
        "        # for N layers\n",
        "        for _ in range(self.num_layers):\n",
        "            # Pre-layer normalization:\n",
        "            # nn.LayerNorm()(x) normalizes each token's 256-d vector\n",
        "            # Shape remains (8, 10, 256)\n",
        "            x_norm = nn.LayerNorm()(x)\n",
        "\n",
        "            # Self-Attention Block with Residual Connection\n",
        "            # - Input: x_norm (8, 10, 256)\n",
        "            # - Internally, each head projects to a 32-d space\n",
        "            #   Thus, for 8 heads: total dimension = 8 * 32 = 256\n",
        "            # - The attention mechanism outputs a tensor of shape (8, 10, 256)\n",
        "            # - The causal mask ensures each position only attends to previous ones\n",
        "            # !!! output matches the initial x, the process repeats num_layers times\n",
        "            attn_out = nn.MultiHeadDotProductAttention(\n",
        "                num_heads=self.num_heads,\n",
        "                qkv_features=self.head_size,\n",
        "                out_features=self.head_size * self.num_heads,\n",
        "                dropout_rate=self.dropout_rate,\n",
        "            )(\n",
        "                x_norm,  # queries: (8, 10, 256)\n",
        "                x_norm,  # keys:    (8, 10, 256)\n",
        "                mask=jnp.tril(jnp.ones((x.shape[-2], x.shape[-2]))),  # (10, 10)\n",
        "                deterministic=not training,\n",
        "            )\n",
        "\n",
        "            # Residual connection: add attention output to original x\n",
        "            # Shape remains (8, 10, 256)\n",
        "            x = x + attn_out\n",
        "\n",
        "            # Feedforward (MLP) Block with Residual Connection\n",
        "            # extra \"x +\" helps with gradient flow and retains information from earlier layers\n",
        "            x = x + nn.Sequential([\n",
        "                nn.Dense(4 * self.embed_size), # Expand: (8, 10, 256) -> (8, 10, 1024)\n",
        "                nn.relu, # Activation: (8, 10, 1024)\n",
        "                nn.Dropout(self.dropout_rate, deterministic=not training), # (8, 10, 1024)\n",
        "                nn.Dense(self.embed_size), # Project: (8, 10, 1024) -> (8, 10, 256)\n",
        "            ])(nn.LayerNorm()(x))\n",
        "\n",
        "        # Final Layer Normalization:\n",
        "        # Normalizes final representation at each token\n",
        "        # Shape: (8, 10, 256)\n",
        "        x = nn.LayerNorm()(x)\n",
        "\n",
        "        # Output Projection:\n",
        "        # Projects each 256-d token representation to logits over the vocabulary\n",
        "        # Dense layer: (8, 10, 256) -> (8, 10, vocab_size) = (8, 10, 100)\n",
        "        return nn.Dense(self.vocab_size)(x)"
      ],
      "metadata": {
        "id": "NdHXRJrteoO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the model and run inference"
      ],
      "metadata": {
        "id": "r8cQ4W8nmmJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model initialization\n",
        "key = jax.random.PRNGKey(1337)\n",
        "mini_transformer = NanoLM(vocab_size=100)\n",
        "\n",
        "# Example input: batch of token sequences (batch_size=8, seq_len=10)\n",
        "x = jnp.ones((8, 10), dtype=jnp.int32)\n",
        "\n",
        "# Initialize parameters\n",
        "params = mini_transformer.init(key, x)\n",
        "\n",
        "# Forward pass\n",
        "y = mini_transformer.apply(params, x, False)\n",
        "\n",
        "# Predict the next token at every (!) position\n",
        "# aka \"teacher forcing\" - helps the model learn the structure of the language by maximizing the likelihood of the entire sequence\n",
        "# if we only produced an output for the end token,\n",
        "# we'd lose valuable learning signals from every intermediate step\n",
        "y.shape"
      ],
      "metadata": {
        "id": "7YgGcotVe8Pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_params(params):\n",
        "    leaves = jax.tree_util.tree_leaves(params)\n",
        "    return sum(x.size for x in leaves)\n",
        "\n",
        "n_params = count_params(params)\n",
        "print(f\"Total number of parameters: {n_params:,}\")"
      ],
      "metadata": {
        "id": "TfVuMakSwBnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def params_size_in_bytes(params):\n",
        "    leaves = jax.tree_util.tree_leaves(params)\n",
        "    total_bytes = sum([x.size * x.dtype.itemsize for x in leaves])\n",
        "    return total_bytes\n",
        "\n",
        "size_bytes = params_size_in_bytes(params)\n",
        "print(\"Total parameters size: {:.2f} MB\".format(size_bytes / (1024 ** 2)))"
      ],
      "metadata": {
        "id": "sKgG9sUYvy-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Makes sense, since with 3,426,468 parameters and assuming 32-bit (4-byte) floats, the total parameter memory is roughly:\n",
        "\n",
        "$$\n",
        "3,426,468 \\times 4 \\text{ bytes} \\approx 13,705,872 \\text{ bytes} \\approx 13.07 \\text{ MB}\n",
        "$$"
      ],
      "metadata": {
        "id": "BQGQjg50w02X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resume**:\n",
        "\n",
        "<img src=\"https://pbs.twimg.com/media/GCnZNRraAAE9HAx?format=png&name=small\" width=\"600\">\n",
        "\n",
        "Source: https://x.com/srush_nlp/status/1741161984928920027"
      ],
      "metadata": {
        "id": "9XUHlv7hhMog"
      }
    }
  ]
}