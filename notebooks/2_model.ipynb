{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "7_PA03GbeyIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note! Ensure Runtime GPU is used."
      ],
      "metadata": {
        "id": "-6elihvG-kS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anatomy of the Model"
      ],
      "metadata": {
        "id": "ZNP3CvLjeyS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the required packages"
      ],
      "metadata": {
        "id": "SHFmRS8zeyVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install flax"
      ],
      "metadata": {
        "id": "Q30A61o_e4Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the imports"
      ],
      "metadata": {
        "id": "ji1Y9b6efPGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from functools import partial\n",
        "from typing import Any, Callable\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax\n",
        "from flax import linen as nn"
      ],
      "metadata": {
        "id": "EtWrCIJde4W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model signature"
      ],
      "metadata": {
        "id": "PuFX0VcgfZyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ f(w; x) = \\hat{y} $$\n",
        "Here we place parameters at the first place to match the signature required later by JAX."
      ],
      "metadata": {
        "id": "XxPDzUYrcHGe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3gD_xPgzqDN"
      },
      "outputs": [],
      "source": [
        "# Linear Regression\n",
        "np.random.seed(1337)\n",
        "\n",
        "def predict(p, x):\n",
        "  y = p.T @ x\n",
        "  return y\n",
        "\n",
        "params = np.random.standard_normal(5)\n",
        "\n",
        "# features, batch of data\n",
        "x = np.array([1] + [2, 3, 7, 2])\n",
        "\n",
        "# output\n",
        "y = predict(params, x)\n",
        "\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP model signature"
      ],
      "metadata": {
        "id": "5FvWhS_odj-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-layer Dense network"
      ],
      "metadata": {
        "id": "_PZy-DdU3zJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import HTML\n",
        "\n",
        "url = \"https://www.researchgate.net/publication/221079407/figure/fig1/AS:651187686744067@1532266651725/One-layer-neural-network-and-nomenclature-employed.png\"\n",
        "display(HTML(f'<img src=\"{url}\" width=\"500px\">'))  # Adjust width as needed\n"
      ],
      "metadata": {
        "id": "9_GTVI6Z29gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(W, b, x):\n",
        "    z = W @ x + b   # Linear transformation\n",
        "    a = np.maximum(0, z)  # ReLU activation\n",
        "    return a\n",
        "\n",
        "input_dim = 4  # Input features\n",
        "output_dim = 1  # Number of output neurons\n",
        "\n",
        "x = np.array([2, 3, 7, 2])\n",
        "\n",
        "W = np.random.randn(output_dim, input_dim)  # Random weights\n",
        "b = np.random.randn(output_dim, )  # Random biases\n",
        "\n",
        "y = predict(W, b, x)\n",
        "\n",
        "y"
      ],
      "metadata": {
        "id": "v4A9AReadi7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP in JAX/Flax"
      ],
      "metadata": {
        "id": "6j1Moeo0djkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flax Model API:\n",
        "\n",
        "1️⃣ **Define the model** (`nn.Module`, (optionally) with `setup()`)  \n",
        "2️⃣ **Initialize parameters** (`model.init()`)  \n",
        "3️⃣ **Run inference** (`model.apply()`)  \n"
      ],
      "metadata": {
        "id": "m6uWuQ3eeSsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "import jax\n",
        "\n",
        "# Define the predict function using JAX\n",
        "def predict(W, b, x):\n",
        "    z = W @ x + b  # Linear transformation\n",
        "    a = jnp.maximum(0, z)  # ReLU activation\n",
        "    return a\n",
        "\n",
        "# Define input and output dimensions\n",
        "input_dim = 4\n",
        "output_dim = 1\n",
        "\n",
        "# Initialize weights and biases with random values\n",
        "key = jax.random.PRNGKey(0)  # JAX requires a PRNG key for randomness\n",
        "W = jax.random.normal(key, (output_dim, input_dim))  # Random weights\n",
        "b = jax.random.normal(key, (output_dim, ))  # Random biases\n",
        "\n",
        "# Define input vector\n",
        "x = jnp.array([2, 3, 7, 2])\n",
        "\n",
        "# Predict\n",
        "y = predict(W, b, x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "U7_kroUoedAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import flax.linen as nn\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    output_dim: int  # Number of output neurons\n",
        "\n",
        "    def setup(self):\n",
        "        self.dense = nn.Dense(self.output_dim)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        z = self.dense(x)  # Linear transformation\n",
        "        return nn.relu(z)  # ReLU activation\n",
        "\n",
        "# Define input and output dimensions\n",
        "input_dim = 4\n",
        "output_dim = 1\n",
        "\n",
        "# Define input vector\n",
        "x = jnp.array([2, 3, 7, 2])\n",
        "\n",
        "# Create model instance\n",
        "model = SimpleNN(output_dim=output_dim)\n",
        "\n",
        "# Initialize parameters\n",
        "key = jax.random.PRNGKey(0)\n",
        "params = model.init(key, jnp.ones(input_dim))  # Initialize with dummy input\n",
        "\n",
        "# Run inference\n",
        "y = model.apply(params, x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "HojXQSOg5X24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bookkeeping"
      ],
      "metadata": {
        "id": "_DFwnwGN6NSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **Flax**, model parameters (`params`) are stored as a **frozen dictionary (`FrozenDict`)**, which can be **saved and loaded** using JAX serialization tools like `flax.serialization.to_bytes()` and `flax.serialization.from_bytes()`, or `pickle`/`json` for more flexibility.\n",
        "\n",
        "**1️⃣ Save Model Weights to a File**\n",
        "```python\n",
        "import flax\n",
        "import pickle\n",
        "\n",
        "# Save params to a file (binary format)\n",
        "with open(\"model_params.pkl\", \"wb\") as f:\n",
        "    pickle.dump(flax.serialization.to_bytes(params), f)\n",
        "```\n",
        "\n",
        "**2️⃣ Load Model Weights from a File**\n",
        "```python\n",
        "# Load params from file\n",
        "with open(\"model_params.pkl\", \"rb\") as f:\n",
        "    params_loaded = flax.serialization.from_bytes(params, pickle.load(f))\n",
        "\n",
        "print(\"Loaded Parameters:\", params_loaded)\n",
        "```\n"
      ],
      "metadata": {
        "id": "mho3hfCK6WSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"model_params.pkl\", \"wb\") as f:\n",
        "    pickle.dump(flax.serialization.to_bytes(params), f)"
      ],
      "metadata": {
        "id": "DwjJjWBP6OA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"model_params.pkl\", \"rb\") as f:\n",
        "    params_loaded = flax.serialization.from_bytes(params, pickle.load(f))"
      ],
      "metadata": {
        "id": "Kw2tk4gg66ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run inference again\n",
        "y = model.apply(params_loaded, x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "ZCyYTikz6-bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention and Transformer"
      ],
      "metadata": {
        "id": "D71_dJG1enrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini Transformer in Flax"
      ],
      "metadata": {
        "id": "AMd-Guea016M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NanoLM(nn.Module):\n",
        "    vocab_size: int\n",
        "    num_layers: int = 6\n",
        "    num_heads: int = 8\n",
        "    head_size: int = 32\n",
        "    dropout_rate: float = 0.2\n",
        "    embed_size: int = 256\n",
        "    block_size: int = 64\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, training: bool = True):\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        x = nn.Embed(self.vocab_size, self.embed_size)(x) + \\\n",
        "            nn.Embed(self.block_size, self.embed_size)(jnp.arange(seq_len))\n",
        "\n",
        "        for _ in range(self.num_layers):\n",
        "            x_norm = nn.LayerNorm()(x)\n",
        "\n",
        "            x = x + nn.MultiHeadDotProductAttention(\n",
        "                num_heads=self.num_heads,\n",
        "                qkv_features=self.head_size,\n",
        "                out_features=self.head_size * self.num_heads,\n",
        "                dropout_rate=self.dropout_rate,\n",
        "            )(\n",
        "                x_norm,\n",
        "                x_norm,\n",
        "                mask=jnp.tril(jnp.ones((x.shape[-2], x.shape[-2]))),\n",
        "                deterministic=not training,\n",
        "            )\n",
        "\n",
        "            x = x + nn.Sequential([\n",
        "                nn.Dense(4 * self.embed_size),\n",
        "                nn.relu,\n",
        "                nn.Dropout(self.dropout_rate, deterministic=not training),\n",
        "                nn.Dense(self.embed_size),\n",
        "            ])(nn.LayerNorm()(x))\n",
        "\n",
        "        x = nn.LayerNorm()(x)\n",
        "\n",
        "        return nn.Dense(self.vocab_size)(x)"
      ],
      "metadata": {
        "id": "NdHXRJrteoO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model initialization\n",
        "key = jax.random.PRNGKey(1337)\n",
        "mini_transformer = NanoLM(vocab_size=100)\n",
        "\n",
        "# Example input: batch of token sequences (batch_size=1, seq_len=10)\n",
        "x = jnp.ones((1, 10), dtype=jnp.int32)\n",
        "\n",
        "# Initialize parameters\n",
        "params = mini_transformer.init(key, x)\n",
        "\n",
        "# Forward pass\n",
        "y = mini_transformer.apply(params, x, False)\n",
        "\n",
        "y.shape"
      ],
      "metadata": {
        "id": "7YgGcotVe8Pb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}