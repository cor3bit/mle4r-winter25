{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "7_PA03GbeyIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training an MLP model"
      ],
      "metadata": {
        "id": "ZNP3CvLjeyS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the required packages"
      ],
      "metadata": {
        "id": "SHFmRS8zeyVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install flax wandb tensorboardX tiktoken"
      ],
      "metadata": {
        "id": "Q30A61o_e4Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the imports"
      ],
      "metadata": {
        "id": "ji1Y9b6efPGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.datasets as skdata\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import flax\n",
        "from flax import linen as nn"
      ],
      "metadata": {
        "id": "EtWrCIJde4W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and preprocess the Iris dataset."
      ],
      "metadata": {
        "id": "G9g6SiQFK3nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = skdata.load_iris()\n",
        "X = iris.data  # shape: (150, 4)\n",
        "y = iris.target  # Labels: 0, 1, 2\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1337,\n",
        ")\n",
        "\n",
        "# Convert to JAX arrays\n",
        "X_train = jnp.array(X_train)\n",
        "y_train = jnp.array(y_train)"
      ],
      "metadata": {
        "id": "xzOMfk5CK2rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a model"
      ],
      "metadata": {
        "id": "m4tF1vMQKjJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPClassifierSmall(nn.Module):\n",
        "    num_classes: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x: jnp.ndarray):\n",
        "        x = nn.Dense(8)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(16)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(8)(x)\n",
        "        x = nn.relu(x)\n",
        "        x = nn.Dense(self.num_classes)(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "oZn4fyESJiW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, run a script!"
      ],
      "metadata": {
        "id": "SKWDMrbWLbz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HPs\n",
        "num_epochs = 100\n",
        "batch_size = 16\n",
        "learning_rate = 1e-3\n",
        "num_classes = 3\n",
        "input_features = X.shape[1]\n",
        "\n",
        "# Initialize the model\n",
        "rng = jax.random.PRNGKey(0)\n",
        "model = MLPClassifierSmall(num_classes=num_classes)\n",
        "params = model.init(rng, jnp.ones((1, input_features)))\n",
        "\n",
        "# Set up the optimizer\n",
        "optimizer = optax.adam(learning_rate)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "# Define the loss function\n",
        "def loss_fn(params, x, y):\n",
        "    logits = model.apply(params, x)\n",
        "    one_hot = jax.nn.one_hot(y, num_classes)\n",
        "    loss = optax.softmax_cross_entropy(logits, one_hot).mean()\n",
        "    return loss\n",
        "\n",
        "@jax.jit\n",
        "def accuracy(params, x, y):\n",
        "    logits = model.apply(params, x)\n",
        "    predicted_classes = jnp.argmax(logits, axis=1)\n",
        "    correct_predictions = predicted_classes == y\n",
        "    return jnp.mean(correct_predictions)\n",
        "\n",
        "\n",
        "# A single update step\n",
        "@jax.jit\n",
        "def update(params, opt_state, x, y):\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, opt_state, loss\n",
        "\n",
        "num_train = X_train.shape[0]\n",
        "num_test = X_test.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "print(f\"Accuracy before training: {accuracy(params, X_test, y_test)}\")\n",
        "\n",
        "# Training loop!\n",
        "for epoch in range(num_epochs):\n",
        "    # Shuffle training data\n",
        "    permutation = jax.random.permutation(rng, num_train)\n",
        "    X_train_shuffled = X_train[permutation]\n",
        "    y_train_shuffled = y_train[permutation]\n",
        "\n",
        "    epoch_train_loss = 0.0\n",
        "\n",
        "    # Process training batches\n",
        "    for i in range(0, num_train, batch_size):\n",
        "        batch_x = X_train_shuffled[i:i+batch_size]\n",
        "        batch_y = y_train_shuffled[i:i+batch_size]\n",
        "        params, opt_state, loss = update(params, opt_state, batch_x, batch_y)\n",
        "        epoch_train_loss += loss * batch_x.shape[0]\n",
        "\n",
        "    epoch_train_loss /= num_train\n",
        "    train_losses.append(float(epoch_train_loss))\n",
        "\n",
        "print(f\"Accuracy after training: {accuracy(params, X_test, y_test)}\")\n",
        "\n",
        "# Plot training vs testing loss.\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, num_epochs+1), train_losses, label=\"Train Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qzeu73yxHkq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tracking"
      ],
      "metadata": {
        "id": "fzDRW-1tHgMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tqdm - your friendly neighborhood progress bar"
      ],
      "metadata": {
        "id": "TAQOSf5oOg9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`tqdm` is lightweight a Python package that provides **fast, extensible progress bars** for loops and iterative processes—**extremely useful in ML workflows** for tracking training, data loading, and hyperparameter tuning.  \n",
        "\n",
        "✅ **Real-time feedback** → See how long each epoch/batch takes.  \n",
        "✅ **ETA estimation** → Know how much time is left for training.  \n",
        "✅ **Seamless integration** → Works with **loops, DataLoaders, and multiprocessing**.  \n",
        "✅ **Minimal performance overhead** → Negligible impact on computation time."
      ],
      "metadata": {
        "id": "MAr0SPzEw0j8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  ------ ONE LINE OF CODE HERE ------\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# HPs\n",
        "num_epochs = 100\n",
        "batch_size = 16\n",
        "learning_rate = 1e-3\n",
        "num_classes = 3\n",
        "input_features = X.shape[1]\n",
        "\n",
        "# Initialize the model\n",
        "rng = jax.random.PRNGKey(0)\n",
        "model = MLPClassifierSmall(num_classes=num_classes)\n",
        "params = model.init(rng, jnp.ones((1, input_features)))\n",
        "\n",
        "# Set up the optimizer\n",
        "optimizer = optax.adam(learning_rate)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "# Define the loss function\n",
        "def loss_fn(params, x, y):\n",
        "    logits = model.apply(params, x)\n",
        "    one_hot = jax.nn.one_hot(y, num_classes)\n",
        "    loss = optax.softmax_cross_entropy(logits, one_hot).mean()\n",
        "    return loss\n",
        "\n",
        "@jax.jit\n",
        "def accuracy(params, x, y):\n",
        "    logits = model.apply(params, x)\n",
        "    predicted_classes = jnp.argmax(logits, axis=1)\n",
        "    correct_predictions = predicted_classes == y\n",
        "    return jnp.mean(correct_predictions)\n",
        "\n",
        "\n",
        "# A single update step\n",
        "@jax.jit\n",
        "def update(params, opt_state, x, y):\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, opt_state, loss\n",
        "\n",
        "num_train = X_train.shape[0]\n",
        "num_test = X_test.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "print(f\"Accuracy before training: {accuracy(params, X_test, y_test)}\")\n",
        "\n",
        "# Training loop!\n",
        "#  ------ AND HERE ------\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    # Shuffle training data\n",
        "    permutation = jax.random.permutation(rng, num_train)\n",
        "    X_train_shuffled = X_train[permutation]\n",
        "    y_train_shuffled = y_train[permutation]\n",
        "\n",
        "    epoch_train_loss = 0.0\n",
        "\n",
        "    # Process training batches\n",
        "    for i in range(0, num_train, batch_size):\n",
        "        batch_x = X_train_shuffled[i:i+batch_size]\n",
        "        batch_y = y_train_shuffled[i:i+batch_size]\n",
        "        params, opt_state, loss = update(params, opt_state, batch_x, batch_y)\n",
        "        epoch_train_loss += loss * batch_x.shape[0]\n",
        "\n",
        "    epoch_train_loss /= num_train\n",
        "    train_losses.append(float(epoch_train_loss))\n",
        "\n",
        "print(f\"Accuracy after training: {accuracy(params, X_test, y_test)}\")"
      ],
      "metadata": {
        "id": "qYKsfX11HkoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TensorBoard and Weights & Biases"
      ],
      "metadata": {
        "id": "XDDSqwf4Pb5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TensorBoard** is a visualization toolkit originally developed for TensorFlow but now widely used across ML frameworks. It helps you monitor, debug, and optimize your models. Here's why and how to use it.\n",
        "\n",
        "\n",
        "**How to use TensorBoard:**\n",
        "\n",
        "1. **Logging:**  \n",
        "   In your training loop, log scalar values (like loss and accuracy), histograms, images, or even model graphs. For example, using tensorboardX’s SummaryWriter (or similar for other frameworks):\n",
        "\n",
        "   ```python\n",
        "   from tensorboardX import SummaryWriter\n",
        "   writer = SummaryWriter(log_dir=\"mle4r\")\n",
        "   \n",
        "   # Log a scalar value (convert JAX arrays to float if needed)\n",
        "   writer.add_scalar(\"Loss/Train\", float(train_loss), epoch)\n",
        "   writer.add_scalar(\"Accuracy/Test\", float(test_acc), epoch)\n",
        "   ```\n",
        "\n",
        "2. **Launching TensorBoard:**  \n",
        "   From the command line, run:\n",
        "   ```bash\n",
        "   tensorboard --logdir=runs\n",
        "   ```\n",
        "   Then open the provided URL in a browser to view your metrics."
      ],
      "metadata": {
        "id": "sUATpKF-x9eW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weights & Biases (W&B)** is a powerful experiment tracking and collaboration tool for machine learning. It helps you log metrics, visualize training curves, manage hyperparameters, and compare different runs, all in one centralized dashboard.\n",
        "\n",
        "**How to use W&B:**\n",
        "\n",
        "1. **Initialization:**  \n",
        "   At the beginning of your training script, initialize a run with your project name and configuration. For example:\n",
        "   \n",
        "   ```python\n",
        "   import wandb\n",
        "\n",
        "   # Initialize a new run\n",
        "   wandb.init(\n",
        "    project=\"YOUR_PROJECT\",\n",
        "    \n",
        "    config={\n",
        "       \"num_epochs\": 100,\n",
        "       \"batch_size\": 16,\n",
        "       \"learning_rate\": 1e-3,\n",
        "       \"num_classes\": 3,\n",
        "       \"input_features\": X.shape[1],  # assuming X is defined\n",
        "   })\n",
        "   ```\n",
        "\n",
        "2. **Logging Metrics:**  \n",
        "   In your training loop, log key metrics (like loss, accuracy, etc.) by calling `wandb.log()`. You can log metrics every epoch or even every batch:\n",
        "   \n",
        "   ```python\n",
        "   # Inside your training loop:\n",
        "   wandb.log({\n",
        "       \"epoch\": epoch,\n",
        "       \"train_loss\": float(epoch_train_loss),\n",
        "       \"test_loss\": float(epoch_test_loss),\n",
        "       \"test_accuracy\": float(test_acc)\n",
        "   })\n",
        "   ```\n",
        "\n",
        "3. **Logging Artifacts and Visualizations:**  \n",
        "   W&B allows you to log model artifacts (like trained weights or model files) and visualizations (images, plots, etc.). For example, you might save a plot of training vs. validation loss or upload the model checkpoint.\n",
        "\n",
        "4. **Hyperparameter Sweeps:**  \n",
        "   You can set up sweeps to automatically search through hyperparameter combinations. This helps in automating experiment tracking and finding the best configuration.\n",
        "\n",
        "5. **Dashboard:**  \n",
        "   Once your script is running, you can visit your W&B dashboard in a web browser to see real-time charts, compare different runs, and drill down into the details of each experiment.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zpw0ZJqyx9bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use W&B you should create and account first and use the **W&B API key**.\n",
        "\n",
        "Note: save the key to: Note: Colab -> Secrets"
      ],
      "metadata": {
        "id": "W4urDJPCx9Wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')"
      ],
      "metadata": {
        "id": "PDfxPwayRY0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login $WANDB_API_KEY"
      ],
      "metadata": {
        "id": "HkChCtDmRYxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the full scipt"
      ],
      "metadata": {
        "id": "Q0_yIGu_25wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "from tqdm.notebook import tqdm\n",
        "import wandb\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "# HPs\n",
        "num_epochs = 100\n",
        "batch_size = 16\n",
        "learning_rate = 1e-3\n",
        "num_classes = 3\n",
        "input_features = X.shape[1]\n",
        "\n",
        "# Initialize Weights & Biases\n",
        "wandb.init(\n",
        "    entity=\"dysco\",\n",
        "    project=\"mle4r\",\n",
        "\n",
        "    sync_tensorboard=True,\n",
        "\n",
        "    config={\n",
        "      \"num_epochs\": num_epochs,\n",
        "      \"batch_size\": batch_size,\n",
        "      \"learning_rate\": learning_rate,\n",
        "      \"num_classes\": num_classes,\n",
        "      \"input_features\": input_features,\n",
        "})\n",
        "\n",
        "\n",
        "# Initialize TensorBoardX SummaryWriter\n",
        "writer = SummaryWriter(log_dir=\"mle4r\")\n",
        "\n",
        "# Initialize the model\n",
        "rng = jax.random.PRNGKey(0)\n",
        "model = MLPClassifierSmall(num_classes=num_classes)\n",
        "params = model.init(rng, jnp.ones((1, input_features)))\n",
        "\n",
        "# Set up the optimizer\n",
        "optimizer = optax.adam(learning_rate)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "# Define the loss function\n",
        "def loss_fn(params, x, y):\n",
        "    logits = model.apply(params, x)\n",
        "    one_hot = jax.nn.one_hot(y, num_classes)\n",
        "    loss = optax.softmax_cross_entropy(logits, one_hot).mean()\n",
        "    return loss\n",
        "\n",
        "# Accuracy function\n",
        "@jax.jit\n",
        "def accuracy(params, x, y):\n",
        "    logits = model.apply(params, x)\n",
        "    predicted_classes = jnp.argmax(logits, axis=1)\n",
        "    correct_predictions = predicted_classes == y\n",
        "    return jnp.mean(correct_predictions)\n",
        "\n",
        "# A single update step\n",
        "@jax.jit\n",
        "def update(params, opt_state, x, y):\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params, x, y)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, opt_state, loss\n",
        "\n",
        "# Evaluation function (for test loss)\n",
        "@jax.jit\n",
        "def eval_step(params, x, y):\n",
        "    return loss_fn(params, x, y)\n",
        "\n",
        "num_train = X_train.shape[0]\n",
        "num_test = X_test.shape[0]\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# Log initial test accuracy\n",
        "init_test_acc = accuracy(params, X_test, y_test)\n",
        "print(f\"Accuracy before training: {init_test_acc:.4f}\")\n",
        "\n",
        "writer.add_scalar(\"Test/Accuracy\", init_test_acc, 0)\n",
        "\n",
        "# Training loop!\n",
        "for epoch in tqdm(range(1, num_epochs + 1)):\n",
        "    # Shuffle training data\n",
        "    permutation = jax.random.permutation(rng, num_train)\n",
        "    X_train_shuffled = X_train[permutation]\n",
        "    y_train_shuffled = y_train[permutation]\n",
        "\n",
        "    epoch_train_loss = 0.0\n",
        "\n",
        "    # Process training batches\n",
        "    for i in range(0, num_train, batch_size):\n",
        "        batch_x = X_train_shuffled[i:i+batch_size]\n",
        "        batch_y = y_train_shuffled[i:i+batch_size]\n",
        "        params, opt_state, loss = update(params, opt_state, batch_x, batch_y)\n",
        "        epoch_train_loss += loss * batch_x.shape[0]\n",
        "\n",
        "    epoch_train_loss /= num_train\n",
        "    train_losses.append(float(epoch_train_loss))\n",
        "\n",
        "    # Evaluate on test data (loss)\n",
        "    epoch_test_loss = loss_fn(params, X_test, y_test)\n",
        "    test_losses.append(float(epoch_test_loss))\n",
        "\n",
        "    # Compute test accuracy\n",
        "    test_acc = accuracy(params, X_test, y_test)\n",
        "\n",
        "    # Logging to TensorBoard\n",
        "    writer.add_scalar(\"Train/Loss\", epoch_train_loss, epoch)\n",
        "    writer.add_scalar(\"Test/Loss\", epoch_test_loss, epoch)\n",
        "    writer.add_scalar(\"Test/Accuracy\", test_acc, epoch)\n",
        "\n",
        "    # print(f\"Epoch {epoch:03d}: Train Loss: {epoch_train_loss:.4f}, Test Loss: {epoch_test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "print(f\"Accuracy after training: {accuracy(params, X_test, y_test):.4f}\")\n",
        "\n",
        "# Close the TensorBoard writer when done\n",
        "writer.close()\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "6__IUndNHklc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also analyze runs in TensorBoard directly from **local** data."
      ],
      "metadata": {
        "id": "6e4pYpQ05FT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "tY9dT1n15Enl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=mle4r"
      ],
      "metadata": {
        "id": "yn_Kf2o2HqYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !kill 7436"
      ],
      "metadata": {
        "id": "xFo_X2HK8F9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From Training Loop To Training Script"
      ],
      "metadata": {
        "id": "rhaRmn_FHsNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Wrapping HPs as scripts arguments"
      ],
      "metadata": {
        "id": "7gggJ85kP0TQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`argparse` is a standard Python library that lets you define command line arguments so you can configure your training loop (or any script) without hardcoding hyperparameters. This makes your script flexible and easier to run with different configurations."
      ],
      "metadata": {
        "id": "lCV_dwmO63SM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Train a small MLP classifier.\")\n",
        "\n",
        "    parser.add_argument(\"--num_epochs\", type=int, default=100, help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=16, help=\"Batch size\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=1e-3, help=\"Learning rate\")\n",
        "\n",
        "    # !! Colab Fix: ignore unknown arguments\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "\n",
        "    print(args.num_epochs)\n",
        "    print(args.batch_size)\n",
        "    print(args.learning_rate)"
      ],
      "metadata": {
        "id": "pxzAt4qOJhAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mini-Transformer on tiny_shakespeare"
      ],
      "metadata": {
        "id": "bOxSarrdP6pW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1. Download from GitHub"
      ],
      "metadata": {
        "id": "4yJ2I1Nm29as"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/"
      ],
      "metadata": {
        "id": "GTJjuOYLFUGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# clean previous files\n",
        "!rm -rf mle4r-winter25\n",
        "\n",
        "# add new ones - EGN\n",
        "!git clone https://github.com/cor3bit/mle4r-winter25.git"
      ],
      "metadata": {
        "id": "0m6NcQToJg60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2. Let's visualize the script"
      ],
      "metadata": {
        "id": "dCAiJJ0w3Aiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See in Colab Editor"
      ],
      "metadata": {
        "id": "LaLal2FCGrGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd mle4r-winter25/scripts\n",
        "# %ls"
      ],
      "metadata": {
        "id": "fop7o0XLJg4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3. Run the script"
      ],
      "metadata": {
        "id": "_7ER6Mdf3Dgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python mle4r-winter25/scripts/train_lm.py --learning-rate 0.001"
      ],
      "metadata": {
        "id": "kF0vbjEsQZPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HPOpt: Grid Search with `subprocess` scripting"
      ],
      "metadata": {
        "id": "k4HBoTrtQY38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the training script with different values of HPs"
      ],
      "metadata": {
        "id": "Ex0HZJ813JFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the hyper-parameter grid\n",
        "batch_size_options = [64, 128]\n",
        "learning_rate_options = [0.001, 0.0001]\n",
        "\n",
        "# Loop over combinations\n",
        "for batch_size in batch_size_options:\n",
        "    for lr in learning_rate_options:\n",
        "        cmd = [\n",
        "            \"python\", \"mle4r-winter25/scripts/train_lm.py\",\n",
        "            \"--batch-size\", str(batch_size),\n",
        "            \"--learning-rate\", str(lr),\n",
        "        ]\n",
        "        print(\"Running:\", \" \".join(cmd))\n",
        "\n",
        "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "\n",
        "        # Print output line by line in real time\n",
        "        # for line in process.stdout:\n",
        "        #     print(line, end='')\n",
        "        process.wait()"
      ],
      "metadata": {
        "id": "w9nJEyVfQaKy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}